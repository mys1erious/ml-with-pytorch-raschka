{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-27T08:20:05.019591Z",
     "start_time": "2024-07-27T08:14:40.151616Z"
    }
   },
   "source": [
    "import torchvision\n",
    "\n",
    "image_path = '../datasets/'\n",
    "\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path,\n",
    "    split='train',\n",
    "    target_type='attr',\n",
    "    download=True\n",
    ")\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    image_path,\n",
    "    split='valid',\n",
    "    target_type='attr',\n",
    "    download=True\n",
    ")\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    image_path,\n",
    "    split='test',\n",
    "    target_type='attr',\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))\n",
    "print('Test set:', len(celeba_test_dataset))"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:24:08.168143Z",
     "start_time": "2024-07-27T08:24:07.053844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data augmentation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8.5))\n",
    "## Column 1: cropping to a bounding-box\n",
    "ax = fig.add_subplot(2, 5, 1)\n",
    "img, attr = celeba_train_dataset[0]\n",
    "ax.set_title('Crop to a \\nbounding-box', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "img_cropped = transforms.functional.crop(img, 50, 20, 128, 128)\n",
    "ax.imshow(img_cropped)\n",
    "\n",
    "## Column 2: flipping (horizontally)\n",
    "ax = fig.add_subplot(2, 5, 2)\n",
    "img, attr = celeba_train_dataset[1]\n",
    "ax.set_title('Flip (horizontal)', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 7)\n",
    "img_flipped = transforms.functional.hflip(img)\n",
    "ax.imshow(img_flipped)\n",
    "\n",
    "## Column 3: adjust contrast\n",
    "ax = fig.add_subplot(2, 5, 3)\n",
    "img, attr = celeba_train_dataset[2]\n",
    "ax.set_title('Adjust constrast', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 8)\n",
    "img_adj_contrast = transforms.functional.adjust_contrast(\n",
    "    img, contrast_factor=2\n",
    ")\n",
    "ax.imshow(img_adj_contrast)\n",
    "\n",
    "## Column 4: adjust brightness\n",
    "ax = fig.add_subplot(2, 5, 4)\n",
    "img, attr = celeba_train_dataset[3]\n",
    "ax.set_title('Adjust brightness', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 9)\n",
    "img_adj_brightness = transforms.functional.adjust_brightness(\n",
    "    img, brightness_factor=1.3\n",
    ")\n",
    "ax.imshow(img_adj_brightness)\n",
    "\n",
    "## Column 5: cropping from image center\n",
    "ax = fig.add_subplot(2, 5, 5)\n",
    "img, attr = celeba_train_dataset[4]\n",
    "ax.set_title('Center crop\\nand resize', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "img_center_crop = transforms.functional.center_crop(\n",
    "    img, [0.7 * 218, 0.7 * 178]\n",
    ")\n",
    "img_resized = transforms.functional.resize(\n",
    "    img_center_crop, size=(218, 178)\n",
    ")\n",
    "ax.imshow(img_resized)\n",
    "plt.show()"
   ],
   "id": "7d2b125a09dcd728",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:27:16.469323Z",
     "start_time": "2024-07-27T08:27:15.386493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "for i, (img, attr) in enumerate(celeba_train_dataset):\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 1)\n",
    "    ax.imshow(img)\n",
    "    if i == 0:\n",
    "        ax.set_title('Orig.', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 2)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.RandomCrop([178, 178])\n",
    "    ])\n",
    "    img_cropped = img_transform(img)\n",
    "    ax.imshow(img_cropped)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 1: Random crop', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 3)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "    img_flip = img_transform(img_cropped)\n",
    "    ax.imshow(img_flip)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 2: Random flip', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i * 4 + 4)\n",
    "    img_resized = transforms.functional.resize(\n",
    "        img_flip, size=(128, 128)\n",
    "    )\n",
    "    ax.imshow(img_resized)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 3: Resize', size=15)\n",
    "    if i == 2:\n",
    "        break\n",
    "plt.show()"
   ],
   "id": "2a6d0f95a2f1259c",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:31:00.203557Z",
     "start_time": "2024-07-27T08:31:00.199647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "get_smile = lambda attr: attr[31]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop([178, 178]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop([178, 178]),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "id": "1cf6176f9e5b9a34",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:33:14.483503Z",
     "start_time": "2024-07-27T08:33:03.855344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path,\n",
    "    split='train',\n",
    "    target_type='attr',\n",
    "    download=False,\n",
    "    transform=transform_train,\n",
    "    target_transform=get_smile\n",
    ")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "data_loader = DataLoader(celeba_train_dataset, batch_size=2)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "num_epochs = 5\n",
    "for j in range(num_epochs):\n",
    "    img_batch, label_batch = next(iter(data_loader))\n",
    "    img = img_batch[0]\n",
    "    ax = fig.add_subplot(2, 5, j + 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'Epoch {j}:', size=15)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    \n",
    "    img = img_batch[1]\n",
    "    ax = fig.add_subplot(2, 5, j + 6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ],
   "id": "bcbecb0e1e9fc1c3",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:34:14.804244Z",
     "start_time": "2024-07-27T08:33:54.860974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='valid',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='test',\n",
    "    target_type='attr', download=False,\n",
    "    transform=transform, target_transform=get_smile\n",
    ")"
   ],
   "id": "47901f2acebee848",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:03.338801Z",
     "start_time": "2024-07-27T08:35:03.335384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Subset\n",
    "celeba_train_dataset = Subset(\n",
    "    celeba_train_dataset,\n",
    "    torch.arange(16000),\n",
    ")\n",
    "celeba_valid_dataset = Subset(\n",
    "    celeba_valid_dataset,\n",
    "    torch.arange(1000)\n",
    ")\n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))"
   ],
   "id": "1f3287dfc12dd135",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:36:07.849607Z",
     "start_time": "2024-07-27T08:36:07.845775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(\n",
    "    celeba_train_dataset,\n",
    "    batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    celeba_valid_dataset,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    celeba_test_dataset,\n",
    "    batch_size,\n",
    "    shuffle=False\n",
    ")"
   ],
   "id": "916a681345cc1449",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:47:30.201206Z",
     "start_time": "2024-07-27T08:47:30.159361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module(\n",
    "    'conv1',\n",
    "    nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=32,\n",
    "        kernel_size=3,\n",
    "        padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout1', nn.Dropout(p=0.5))\n",
    "\n",
    "model.add_module(\n",
    "    'conv2',\n",
    "    nn.Conv2d(\n",
    "        in_channels=32,\n",
    "        out_channels=64,\n",
    "        kernel_size=3,\n",
    "        padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout2', nn.Dropout(p=0.5))\n",
    "\n",
    "model.add_module(\n",
    "    'conv3',\n",
    "    nn.Conv2d(\n",
    "        in_channels=64,\n",
    "        out_channels=128,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "    )\n",
    ")\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "model.add_module(\n",
    "    'conv4',\n",
    "    nn.Conv2d(\n",
    "        in_channels=128,\n",
    "        out_channels=256,\n",
    "        kernel_size=3,\n",
    "        padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu4', nn.ReLU())\n",
    "\n",
    "x = torch.ones((4, 3, 64, 64))\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('pool4', nn.AvgPool2d(kernel_size=8))\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('fc', nn.Linear(256, 1))\n",
    "model.add_module('sigmoid', nn.Sigmoid())\n",
    "print(model(x).shape)\n",
    "\n",
    "model"
   ],
   "id": "1cd39741c8f35c2d",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:50:39.821362Z",
     "start_time": "2024-07-27T08:50:39.813944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = loss_fn(pred, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "            is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum()\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                pred = model(x_batch)[:, 0]\n",
    "                loss = loss_fn(pred, y_batch.float())\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = ((pred >= 0.5).float() == y_batch).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch+1} accuracy: '\n",
    "            f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n",
    "            f'{accuracy_hist_valid[epoch]:.4f}'\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        loss_hist_train,\n",
    "        loss_hist_valid,\n",
    "        accuracy_hist_train,\n",
    "        accuracy_hist_valid\n",
    "    )"
   ],
   "id": "6a7db5a41480a578",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:54:38.906979Z",
     "start_time": "2024-07-27T08:51:01.493163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1)\n",
    "# Too long to train on more epochs\n",
    "num_epochs = 3\n",
    "hist = train(model, num_epochs, train_dl, valid_dl)"
   ],
   "id": "8c75bfbd40b1c962",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:57:11.595932Z",
     "start_time": "2024-07-27T08:57:11.379647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "x_arr = np.arange(len(hist[0])) + 1\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Accuracy', size=15)\n",
    "plt.show()"
   ],
   "id": "ec7e628e0af8f272",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:58:35.463700Z",
     "start_time": "2024-07-27T08:57:57.643040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy_test = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "        accuracy_test += is_correct.sum()\n",
    "accuracy_test /= len(test_dl.dataset)\n",
    "print(f'Test accuracy: {accuracy_test:.4f}')"
   ],
   "id": "14d22fdec514cfe1",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T09:07:43.304552Z",
     "start_time": "2024-07-27T09:07:43.041904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = model(x_batch)[:, 0] * 100\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "for j in range(10, 20):\n",
    "    ax = fig.add_subplot(2, 5, j - 10 + 1)\n",
    "    ax.set_xticks([]);\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(x_batch[j].permute(1, 2, 0))\n",
    "    if y_batch[j] == 1:\n",
    "        label = 'Smile'\n",
    "    else:\n",
    "        label = 'Not Smile'\n",
    "    \n",
    "    ax.text(\n",
    "        0.5, -0.15,\n",
    "        f'GT: {label:s}\\nPr(Smile)={pred[j]:.0f}%',\n",
    "        size=16,\n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center',\n",
    "        transform=ax.transAxes\n",
    "    \n",
    "    )\n",
    "plt.show()"
   ],
   "id": "76be625782689fe8",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "ac11710d41ed4cf3",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
