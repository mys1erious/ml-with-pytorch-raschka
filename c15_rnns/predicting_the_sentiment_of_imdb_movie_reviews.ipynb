{
 "cells": [
  {
   "cell_type": "code",
   "id": "3363606e659c3adb",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-28T10:09:46.060700Z",
     "start_time": "2024-07-28T10:09:46.058248Z"
    }
   },
   "source": "import torchtext; torchtext.disable_torchtext_deprecation_warning()",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:09:46.242585Z",
     "start_time": "2024-07-28T10:09:46.233047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:00.246749Z",
     "start_time": "2024-07-28T10:09:59.673399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Create the datasets\n",
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    # cut to make the training faster\n",
    "    list(train_dataset)[:2500], [2000, 500]\n",
    ")"
   ],
   "id": "64efd3313cddf165",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:01.968160Z",
     "start_time": "2024-07-28T10:10:01.733932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find unique tokens (words)\n",
    "\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n",
    "           ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Vocab-size:', len(token_counts))"
   ],
   "id": "5b97a0b582999031",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 23447\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:02.547395Z",
     "start_time": "2024-07-28T10:10:02.428963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoding each unique token into integers\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(),\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)\n",
    "print([vocab[token] for token in [\n",
    "    'this', 'is','an', 'example'\n",
    "]])\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(\n",
    "            text_pipeline(_text),\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    \n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list,\n",
    "        batch_first=True\n",
    "    )\n",
    "    return padded_text_list, label_list, lengths\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ],
   "id": "755959851dce4b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 37, 402]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:03.090056Z",
     "start_time": "2024-07-28T10:10:03.083677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Padding (making the len the same)\n",
    "\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)"
   ],
   "id": "7bc06acdb4fe5647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   11,    21,     7,  ...,     0,     0,     0],\n",
      "        [  756,     9,    73,  ...,     0,     0,     0],\n",
      "        [ 4521,  6588,  1437,  ...,     0,     0,     0],\n",
      "        [13363,  9801,   653,  ...,  9813,  9814, 13380]])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([221, 132, 335, 440])\n",
      "torch.Size([4, 440])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:03.849261Z",
     "start_time": "2024-07-28T10:10:03.841637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ],
   "id": "7735c464dd21dc88",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:04.394028Z",
     "start_time": "2024-07-28T10:10:04.388463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature embedding\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3,\n",
    "    padding_idx=0\n",
    ")\n",
    "\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "print(embedding(text_encoded_input))"
   ],
   "id": "8b4b00dd387b4a12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2835,  0.4542,  0.2693],\n",
      "         [ 1.1386, -0.1532,  0.5672],\n",
      "         [-0.6690,  0.5642,  0.0668],\n",
      "         [-0.0070, -0.4395, -1.8729]],\n",
      "\n",
      "        [[-0.6690,  0.5642,  0.0668],\n",
      "         [-0.5165, -0.9710,  0.2513],\n",
      "         [ 1.1386, -0.1532,  0.5672],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:04.774001Z",
     "start_time": "2024-07-28T10:10:04.762727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Base model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # self.rnn = nn.GRU(\n",
    "        # self.rnn = nn.LSTM(\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        # We use the final hidden state from the last hidden layer\n",
    "        #   as the input to the fully connected layer\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = RNN(64, 32)\n",
    "print(model)\n",
    "model(torch.randn(5, 3, 64))"
   ],
   "id": "ca95868858e637a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0944],\n",
       "        [ 0.5408],\n",
       "        [ 0.2953],\n",
       "        [ 0.0781],\n",
       "        [-0.1059]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:05.263104Z",
     "start_time": "2024-07-28T10:10:05.248235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sentiment analysis model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim, \n",
    "        rnn_hidden_size, \n",
    "        fc_hidden_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim, \n",
    "            rnn_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out,\n",
    "            lengths.cpu().numpy(),\n",
    "            enforce_sorted=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(1)\n",
    "model = RNN(\n",
    "    vocab_size,\n",
    "    embed_dim,\n",
    "    rnn_hidden_size,\n",
    "    fc_hidden_size,\n",
    ")\n",
    "model"
   ],
   "id": "ecefcb251002bd06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(23449, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:10:05.827221Z",
     "start_time": "2024-07-28T10:10:05.821218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch\n",
    "        ).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return (\n",
    "        total_acc / len(dataloader.dataset),\n",
    "        total_loss / len(dataloader.dataset)\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "\n",
    "    return (\n",
    "        total_acc / len(dataloader.dataset),\n",
    "        total_loss / len(dataloader.dataset)\n",
    "    )\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "33ec93dc6629af73",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:12:28.612946Z",
     "start_time": "2024-07-28T10:10:07.156942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 3\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl, optimizer, loss_fn)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl, loss_fn)\n",
    "    print(\n",
    "        f'Epoch {epoch} accuracy: {acc_train:.4f}',\n",
    "        f' val_accuracy: {acc_valid:.4f}'\n",
    "    )"
   ],
   "id": "9b546e4d67aa9c12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.8580  val_accuracy: 1.0000\n",
      "Epoch 1 accuracy: 1.0000  val_accuracy: 1.0000\n",
      "Epoch 2 accuracy: 1.0000  val_accuracy: 1.0000\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc_test, _ = evaluate(test_dl, loss_fn)\n",
    "print(f'test_accuracy: {acc_test:.4f}')"
   ],
   "id": "6d50401f1332d41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6e9600c7caafecbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
